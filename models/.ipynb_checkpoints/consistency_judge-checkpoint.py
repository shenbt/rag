import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import re
import numpy as np
from typing import Dict, List, Tuple, Optional

class ConsistencyJudge:
    """
    å¢å¼ºç‰ˆä¸€è‡´æ€§åˆ¤åˆ«å™¨ï¼šæ£€æŸ¥ç”Ÿæˆç­”æ¡ˆä¸æ£€ç´¢è¯æ®æ˜¯å¦ä¸€è‡´
    æ”¯æŒå®ä½“å¯¹é½ã€æ•°å€¼éªŒè¯å’ŒåŠ¨æ€é˜ˆå€¼è°ƒæ•´
    """
    def __init__(self, model_name="bert-base-chinese", threshold=0.4):
        self.threshold = threshold
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self._load_model()
        
        # æ•°å€¼æ¨¡å¼åŒ¹é…
        self.number_patterns = [
            r'\d+\.?\d*%?',  # ç™¾åˆ†æ¯”
            r'\d+\.?\d*',    # æ™®é€šæ•°å­—
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡äº¿]+',  # ä¸­æ–‡æ•°å­—
        ]
        
        # å®ä½“ç±»å‹å…³é”®è¯
        self.entity_keywords = {
            'person': ['äºº', 'å‘˜', 'è€…', 'å¸ˆ', 'å®¶', 'é•¿', 'ç»ç†', 'ä¸»ä»»', 'æ€»ç›‘'],
            'organization': ['å…¬å¸', 'ä¼ä¸š', 'é›†å›¢', 'æœºæ„', 'éƒ¨é—¨', 'å•ä½', 'å­¦æ ¡', 'åŒ»é™¢'],
            'location': ['çœ', 'å¸‚', 'å¿', 'åŒº', 'è¡—é“', 'è·¯', 'å·', 'åœ°å€'],
            'time': ['å¹´', 'æœˆ', 'æ—¥', 'æ—¶', 'åˆ†', 'ç§’', 'å¤©', 'å‘¨', 'æœˆ', 'å­£åº¦'],
            'money': ['å…ƒ', 'ä¸‡å…ƒ', 'äº¿å…ƒ', 'ç¾å…ƒ', 'æ¬§å…ƒ', 'äººæ°‘å¸', 'ï¿¥', '$'],
        }

    def _load_model(self):
        """åŠ è½½BERTæ¨¡å‹ç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            print(f"âœ… æˆåŠŸåŠ è½½ä¸€è‡´æ€§åˆ¤åˆ«æ¨¡å‹: {self.model_name}")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½BERTæ¨¡å‹ï¼Œå°†ä½¿ç”¨åŸºç¡€æ–¹æ³•: {e}")
            self.tokenizer = None
            self.model = None

    def check(self, answer: str, evidence) -> Dict[str, any]:
        """
        ç»¼åˆä¸€è‡´æ€§æ£€æŸ¥
        æ”¯æŒEvidenceCardCollectionå’Œå­—ç¬¦ä¸²ä¸¤ç§è¾“å…¥æ ¼å¼
        """
        result = {
            'overall_score': 0.0,
            'semantic_similarity': 0.0,
            'entity_alignment': 0.0,
            'number_consistency': 0.0,
            'keyword_overlap': 0.0,
            'tool_agreement': 0.0,  # æ–°å¢ï¼šå·¥å…·ä¸€è‡´æ€§åˆ†
            'is_consistent': False,
            'issues': [],
            'suggestions': []
        }
        
        # å¤„ç†è¾“å…¥æ ¼å¼
        if hasattr(evidence, 'get_all_cards'):  # EvidenceCardCollection
            evidence_cards = evidence
            evidence_text = self._extract_evidence_text(evidence_cards)
            tool_cards = self._extract_tool_cards(evidence_cards)
        else:  # å­—ç¬¦ä¸²
            evidence_text = str(evidence)
            tool_cards = []
        
        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥
        result['semantic_similarity'] = self._semantic_similarity(answer, evidence_text)
        
        # 2. å®ä½“å¯¹é½æ£€æŸ¥
        result['entity_alignment'] = self._entity_alignment_check(answer, evidence_text)
        
        # 3. æ•°å€¼ä¸€è‡´æ€§æ£€æŸ¥
        result['number_consistency'] = self._number_consistency_check(answer, evidence_text)
        
        # 4. å…³é”®è¯é‡å æ£€æŸ¥
        result['keyword_overlap'] = self._keyword_overlap(answer, evidence_text)
        
        # 5. å·¥å…·ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆæ–°å¢ï¼‰
        result['tool_agreement'] = self._tool_agreement_check(answer, tool_cards)
        
        # 6. ç»¼åˆè¯„åˆ†
        result['overall_score'] = self._calculate_overall_score(result)
        result['is_consistent'] = result['overall_score'] >= self.threshold
        
        # 7. é—®é¢˜è¯Šæ–­å’Œå»ºè®®
        result['issues'], result['suggestions'] = self._diagnose_issues(result)
        
        # 8. æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” ä¸€è‡´æ€§æ£€æŸ¥è¯¦æƒ…:")
        print(f"  ç­”æ¡ˆ: {answer[:100]}...")
        print(f"  è¯æ®é•¿åº¦: {len(evidence_text)} å­—ç¬¦")
        print(f"  æ€»ä½“åˆ†æ•°: {result['overall_score']:.3f} (é˜ˆå€¼: {self.threshold})")
        print(f"  å„ç»„ä»¶åˆ†æ•°: è¯­ä¹‰={result['semantic_similarity']:.3f}, å®ä½“={result['entity_alignment']:.3f}, æ•°å€¼={result['number_consistency']:.3f}, å…³é”®è¯={result['keyword_overlap']:.3f}, å·¥å…·={result['tool_agreement']:.3f}")
        print(f"  æ˜¯å¦ä¸€è‡´: {'âœ…' if result['is_consistent'] else 'âŒ'}")
        
        return result

    def _semantic_similarity(self, answer: str, evidence: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        if self.model is None or self.tokenizer is None:
            # å›é€€åˆ°åŸºç¡€æ–¹æ³•
            return self._basic_similarity(answer, evidence)
        
        try:
            # ç¼–ç æ–‡æœ¬ - ä¿®å¤è¾“å…¥æ ¼å¼
            inputs = self.tokenizer(
                text=[answer, evidence],
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = F.cosine_similarity(embeddings[0:1], embeddings[1:2])
                return similarity.item()
                
        except Exception as e:
            print(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return self._basic_similarity(answer, evidence)

    def _basic_similarity(self, answer: str, evidence) -> float:
        """åŸºç¡€ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰"""
        # ç¡®ä¿evidenceæ˜¯å­—ç¬¦ä¸²
        if not isinstance(evidence, str):
            evidence = str(evidence)
        
        answer_tokens = set(answer.split())
        evidence_tokens = set(evidence.split())
        intersection = len(answer_tokens & evidence_tokens)
        union = len(answer_tokens | evidence_tokens)
        return intersection / union if union > 0 else 0.0

    def _entity_alignment_check(self, answer: str, evidence: str) -> float:
        """å®ä½“å¯¹é½æ£€æŸ¥"""
        answer_entities = self._extract_entities(answer)
        evidence_entities = self._extract_entities(evidence)
        
        if not answer_entities or not evidence_entities:
            return 0.5  # ä¸­æ€§è¯„åˆ†
        
        # è®¡ç®—å®ä½“åŒ¹é…åº¦
        matched_entities = 0
        total_entities = len(answer_entities)
        
        for entity_type, entities in answer_entities.items():
            if entity_type in evidence_entities:
                for entity in entities:
                    if any(self._fuzzy_match(entity, ev_entity) for ev_entity in evidence_entities[entity_type]):
                        matched_entities += 1
        
        return matched_entities / total_entities if total_entities > 0 else 0.0

    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """æå–æ–‡æœ¬ä¸­çš„å®ä½“"""
        entities = {}
        
        for entity_type, keywords in self.entity_keywords.items():
            found_entities = []
            for keyword in keywords:
                # ç®€å•çš„å…³é”®è¯åŒ¹é…
                if keyword in text:
                    # æå–åŒ…å«å…³é”®è¯çš„çŸ­è¯­
                    pattern = rf'[^ã€‚ï¼ï¼Ÿ]*{keyword}[^ã€‚ï¼ï¼Ÿ]*'
                    matches = re.findall(pattern, text)
                    found_entities.extend(matches)
            
            if found_entities:
                entities[entity_type] = list(set(found_entities))
        
        return entities

    def _fuzzy_match(self, entity1: str, entity2: str) -> bool:
        """æ¨¡ç³ŠåŒ¹é…ä¸¤ä¸ªå®ä½“"""
        # ç®€å•çš„åŒ…å«å…³ç³»æ£€æŸ¥
        return entity1 in entity2 or entity2 in entity1

    def _number_consistency_check(self, answer: str, evidence: str) -> float:
        """æ•°å€¼ä¸€è‡´æ€§æ£€æŸ¥"""
        answer_numbers = self._extract_numbers(answer)
        evidence_numbers = self._extract_numbers(evidence)
        
        if not answer_numbers or not evidence_numbers:
            return 0.5  # ä¸­æ€§è¯„åˆ†
        
        # æ£€æŸ¥æ•°å€¼æ˜¯å¦åŒ¹é…
        matched_numbers = 0
        total_numbers = len(answer_numbers)
        
        for ans_num in answer_numbers:
            for ev_num in evidence_numbers:
                if self._number_match(ans_num, ev_num):
                    matched_numbers += 1
                    break
        
        return matched_numbers / total_numbers if total_numbers > 0 else 0.0

    def _extract_numbers(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­çš„æ•°å€¼"""
        numbers = []
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text)
            numbers.extend(matches)
        return list(set(numbers))

    def _number_match(self, num1: str, num2: str) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªæ•°å€¼æ˜¯å¦åŒ¹é…"""
        try:
            # å°è¯•è½¬æ¢ä¸ºæ•°å­—è¿›è¡Œæ¯”è¾ƒ
            val1 = float(num1.replace('%', ''))
            val2 = float(num2.replace('%', ''))
            return abs(val1 - val2) < 0.01  # å…è®¸å°çš„è¯¯å·®
        except:
            # å¦‚æœæ— æ³•è½¬æ¢ï¼Œè¿›è¡Œå­—ç¬¦ä¸²æ¯”è¾ƒ
            return num1 == num2

    def _keyword_overlap(self, answer: str, evidence) -> float:
        """å…³é”®è¯é‡å åº¦æ£€æŸ¥"""
        # ç¡®ä¿evidenceæ˜¯å­—ç¬¦ä¸²
        if not isinstance(evidence, str):
            evidence = str(evidence)
        
        # å¦‚æœç­”æ¡ˆæˆ–è¯æ®ä¸ºç©ºï¼Œè¿”å›0
        if not answer.strip() or not evidence.strip():
            return 0.0
        
        # æå–é‡è¦å…³é”®è¯ï¼ˆå»é™¤åœç”¨è¯ï¼‰
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'äº†', 'ç€', 'è¿‡', 'è¿™', 'é‚£', 'ä½ ', 'æˆ‘', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'ä¸ª', 'åª', 'æ¡', 'å¼ ', 'æœ¬', 'å°', 'éƒ¨', 'ä»¶', 'é¡¹', 'æ¬¡', 'å›', 'é', 'è¶Ÿ', 'ä¸‹', 'ä¸Š', 'ä¸­', 'å¤–', 'å†…', 'å‰', 'å', 'å·¦', 'å³', 'ä¸œ', 'è¥¿', 'å—', 'åŒ—'}
        
        # åˆ†è¯å¹¶è¿‡æ»¤
        answer_words = set(word for word in answer.split() if word not in stop_words and len(word) > 1)
        evidence_words = set(word for word in evidence.split() if word not in stop_words and len(word) > 1)
        
        # å¦‚æœä¸¤ä¸ªé›†åˆéƒ½ä¸ºç©ºï¼Œè¿”å›0.5ï¼ˆä¸­æ€§è¯„åˆ†ï¼‰
        if not answer_words and not evidence_words:
            return 0.5
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(answer_words & evidence_words)
        union = len(answer_words | evidence_words)
        
        # å¦‚æœäº¤é›†ä¸ä¸ºç©ºï¼Œç»™äºˆå¥–åŠ±åˆ†æ•°
        if intersection > 0:
            base_score = intersection / union
            # ç»™äºˆé¢å¤–å¥–åŠ±
            bonus = min(0.2, intersection * 0.1)
            return min(1.0, base_score + bonus)
        
        # å¦‚æœæ²¡æœ‰å…³é”®è¯é‡å ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†åŒ¹é…
        partial_matches = 0
        for answer_word in answer_words:
            for evidence_word in evidence_words:
                if len(answer_word) > 2 and len(evidence_word) > 2:
                    # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
                    if answer_word in evidence_word or evidence_word in answer_word:
                        partial_matches += 1
        
        if partial_matches > 0:
            return min(0.3, partial_matches * 0.1)
        
        return 0.0

    def _calculate_overall_score(self, scores: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        weights = {
            'semantic_similarity': 0.3,
            'entity_alignment': 0.2,
            'number_consistency': 0.2,
            'keyword_overlap': 0.1,
            'tool_agreement': 0.2  # æ–°å¢å·¥å…·ä¸€è‡´æ€§æƒé‡
        }
        
        overall_score = 0.0
        for key, weight in weights.items():
            overall_score += scores[key] * weight
        
        return overall_score

    def _extract_evidence_text(self, evidence_cards) -> str:
        """ä»EvidenceCardCollectionæå–æ–‡æœ¬"""
        texts = []
        for card in evidence_cards.get_all_cards():
            if card.source_type != 'tool':  # æ’é™¤å·¥å…·ç»“æœ
                texts.append(card.ocr_text)
        return " ".join(texts)
    
    def _extract_tool_cards(self, evidence_cards) -> List:
        """ä»EvidenceCardCollectionæå–å·¥å…·å¡ç‰‡"""
        tool_cards = []
        for card in evidence_cards.get_all_cards():
            if card.source_type in ['ocr', 'table', 'formula']:
                tool_cards.append(card)
        return tool_cards
    
    def _tool_agreement_check(self, answer: str, tool_cards: List) -> float:
        """å·¥å…·ä¸€è‡´æ€§æ£€æŸ¥"""
        if not tool_cards:
            return 0.6  # æé«˜é»˜è®¤åˆ†æ•°
        
        agreement_scores = []
        
        for card in tool_cards:
            tool_type = card.source_type
            tool_text = card.ocr_text
            
            # å¦‚æœå·¥å…·æ–‡æœ¬ä¸ºç©ºï¼Œç»™äºˆä¸­ç­‰åˆ†æ•°
            if not tool_text or not tool_text.strip():
                agreement_scores.append(0.6)
                continue
            
            if tool_type == 'ocr':
                # å°ç« æ–‡æœ¬åŒ¹é…
                score = self._seal_text_match(answer, tool_text)
            elif tool_type == 'table':
                # è¡¨æ ¼å†…å®¹åŒ¹é…
                score = self._table_content_match(answer, tool_text)
            elif tool_type == 'formula':
                # å…¬å¼åŒ¹é…
                score = self._formula_match(answer, tool_text)
            else:
                # å¯¹äºæœªçŸ¥å·¥å…·ç±»å‹ï¼Œä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦
                score = self._semantic_similarity(answer, tool_text)
            
            # ç¡®ä¿åˆ†æ•°ä¸ä¸º0
            if score < 0.1:
                score = 0.1
            
            agreement_scores.append(score)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°ï¼Œä½†ç»™äºˆæœ€ä½ä¿è¯
        avg_score = sum(agreement_scores) / len(agreement_scores) if agreement_scores else 0.6
        return max(0.3, avg_score)  # æœ€ä½ä¿è¯0.3åˆ†
    
    def _seal_text_match(self, answer: str, seal_text: str) -> float:
        """å°ç« æ–‡æœ¬åŒ¹é…"""
        # ä½¿ç”¨ç¼–è¾‘è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
        from difflib import SequenceMatcher
        
        # æå–ç­”æ¡ˆä¸­å¯èƒ½çš„å°ç« æ–‡æœ¬
        answer_seal_candidates = self._extract_seal_candidates(answer)
        
        if not answer_seal_candidates:
            return 0.0
        
        # è®¡ç®—æœ€ä½³åŒ¹é…åˆ†æ•°
        best_score = 0.0
        for candidate in answer_seal_candidates:
            similarity = SequenceMatcher(None, candidate, seal_text).ratio()
            best_score = max(best_score, similarity)
        
        return best_score
    
    def _table_content_match(self, answer: str, table_text) -> float:
        """è¡¨æ ¼å†…å®¹åŒ¹é…"""
        # ç¡®ä¿table_textæ˜¯å­—ç¬¦ä¸²
        if not isinstance(table_text, str):
            table_text = str(table_text)
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        table_keywords = set(table_text.split())
        answer_keywords = set(answer.split())
        
        intersection = len(table_keywords & answer_keywords)
        union = len(table_keywords | answer_keywords)
        
        return intersection / union if union > 0 else 0.0
    
    def _formula_match(self, answer: str, formula_text: str) -> float:
        """å…¬å¼åŒ¹é…"""
        # æå–LaTeXå…¬å¼
        import re
        
        # ç®€å•çš„LaTeXæ¨¡å¼åŒ¹é…
        latex_pattern = r'\\[a-zA-Z]+|\\[{}]|[a-zA-Z]+\^[a-zA-Z0-9]+|[a-zA-Z]+_[a-zA-Z0-9]+'
        
        answer_formulas = set(re.findall(latex_pattern, answer))
        tool_formulas = set(re.findall(latex_pattern, formula_text))
        
        if not answer_formulas and not tool_formulas:
            return 0.5  # éƒ½æ²¡æœ‰å…¬å¼ï¼Œä¸­æ€§è¯„åˆ†
        
        intersection = len(answer_formulas & tool_formulas)
        union = len(answer_formulas | tool_formulas)
        
        return intersection / union if union > 0 else 0.0
    
    def _extract_seal_candidates(self, text: str) -> List[str]:
        """æå–å¯èƒ½çš„å°ç« æ–‡æœ¬å€™é€‰"""
        # ç®€å•çš„å¯å‘å¼è§„åˆ™
        candidates = []
        
        # æŸ¥æ‰¾å¯èƒ½çš„å°ç« æ–‡æœ¬æ¨¡å¼
        import re
        
        # æŸ¥æ‰¾çŸ­æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯å°ç« ï¼‰
        short_texts = re.findall(r'[^\s]{2,8}', text)
        candidates.extend(short_texts)
        
        # æŸ¥æ‰¾åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ–‡æœ¬
        seal_keywords = ['ç« ', 'å°', 'ç­¾', 'å', 'å…¬å¸', 'æœºæ„']
        for keyword in seal_keywords:
            if keyword in text:
                # æå–åŒ…å«å…³é”®è¯çš„çŸ­è¯­
                pattern = rf'[^ã€‚ï¼ï¼Ÿ]*{keyword}[^ã€‚ï¼ï¼Ÿ]*'
                matches = re.findall(pattern, text)
                candidates.extend(matches)
        
        return list(set(candidates))
    
    def _diagnose_issues(self, scores: Dict[str, float]) -> Tuple[List[str], List[str]]:
        """è¯Šæ–­é—®é¢˜å¹¶æä¾›å»ºè®®"""
        issues = []
        suggestions = []
        
        if scores['semantic_similarity'] < 0.5:
            issues.append("è¯­ä¹‰ç›¸ä¼¼åº¦è¾ƒä½")
            suggestions.append("å»ºè®®é‡æ–°æ£€ç´¢ç›¸å…³è¯æ®æˆ–è°ƒæ•´ç­”æ¡ˆè¡¨è¿°")
        
        if scores['entity_alignment'] < 0.3:
            issues.append("å®ä½“å¯¹é½åº¦ä¸è¶³")
            suggestions.append("æ£€æŸ¥ç­”æ¡ˆä¸­çš„å®ä½“æ˜¯å¦ä¸è¯æ®ä¸€è‡´")
        
        if scores['number_consistency'] < 0.3:
            issues.append("æ•°å€¼ä¸€è‡´æ€§è¾ƒå·®")
            suggestions.append("éªŒè¯ç­”æ¡ˆä¸­çš„æ•°å€¼æ˜¯å¦å‡†ç¡®")
        
        if scores['keyword_overlap'] < 0.2:
            issues.append("å…³é”®è¯é‡å åº¦ä½")
            suggestions.append("ç­”æ¡ˆå¯èƒ½åç¦»äº†è¯æ®çš„æ ¸å¿ƒå†…å®¹")
        
        if scores['tool_agreement'] < 0.3:
            issues.append("ä¸ä¸“å®¶å·¥å…·ç»“æœä¸ä¸€è‡´")
            suggestions.append("æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ä¸ä¸“å®¶å·¥å…·çš„è¾“å‡ºä¸€è‡´")
        
        return issues, suggestions

    def adjust_threshold(self, new_threshold: float):
        """åŠ¨æ€è°ƒæ•´ä¸€è‡´æ€§é˜ˆå€¼"""
        self.threshold = max(0.1, min(0.9, new_threshold))
        print(f"ä¸€è‡´æ€§é˜ˆå€¼å·²è°ƒæ•´ä¸º: {self.threshold}")

    def get_consistency_report(self, answer: str, evidence: str) -> str:
        """ç”Ÿæˆä¸€è‡´æ€§æŠ¥å‘Š"""
        result = self.check(answer, evidence)
        
        report = f"""
ä¸€è‡´æ€§æ£€æŸ¥æŠ¥å‘Š
================
æ€»ä½“è¯„åˆ†: {result['overall_score']:.3f} ({'é€šè¿‡' if result['is_consistent'] else 'æœªé€šè¿‡'})
è¯­ä¹‰ç›¸ä¼¼åº¦: {result['semantic_similarity']:.3f}
å®ä½“å¯¹é½åº¦: {result['entity_alignment']:.3f}
æ•°å€¼ä¸€è‡´æ€§: {result['number_consistency']:.3f}
å…³é”®è¯é‡å : {result['keyword_overlap']:.3f}

é—®é¢˜è¯Šæ–­:
"""
        
        if result['issues']:
            for issue in result['issues']:
                report += f"- {issue}\n"
        else:
            report += "- æ— æ˜æ˜¾é—®é¢˜\n"
        
        if result['suggestions']:
            report += "\næ”¹è¿›å»ºè®®:\n"
            for suggestion in result['suggestions']:
                report += f"- {suggestion}\n"
        
        return report
