import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
import json
import os
# é¿å…å¾ªç¯å¯¼å…¥ï¼Œä½¿ç”¨å»¶è¿Ÿå¯¼å…¥
# from models.evidence_cards import EvidenceCard, EvidenceCardCollection

class HaystackRetriever:
    """
    Haystackè§„æ¨¡åŒ–æ£€ç´¢å™¨ï¼šæ··åˆæ£€ç´¢ + å€™é€‰é‡æ’ï¼Œé€‚é…å¤§è§„æ¨¡æ–‡æ¡£åº“
    """
    def __init__(self, 
                 sparse_model_name="BM25",
                 dense_model_name="./all-MiniLM-L6-v2",
                 cross_encoder_name="./ms-marco-MiniLM-L-6-v2",
                 max_docs=1000):
        self.sparse_model_name = sparse_model_name
        self.dense_model_name = dense_model_name
        self.cross_encoder_name = cross_encoder_name
        self.max_docs = max_docs
        
        # æ–‡æ¡£å­˜å‚¨
        self.documents = []
        self.doc_embeddings = []
        self.doc_metadata = []
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        # æ£€ç´¢å‚æ•°
        self.sparse_top_k = 50
        self.dense_top_k = 50
        self.final_top_k = 10

    def _init_models(self):
        """åˆå§‹åŒ–æ£€ç´¢æ¨¡å‹"""
        # ç¨€ç–æ£€ç´¢æ¨¡å‹ï¼ˆBM25ï¼‰- ç›´æ¥ä½¿ç”¨ç®€åŒ–ç‰ˆé¿å…Haystackå…¼å®¹æ€§é—®é¢˜
        print("ğŸ”„ ä½¿ç”¨ç®€åŒ–ç‰ˆBM25ï¼ˆé¿å…Haystackå…¼å®¹æ€§é—®é¢˜ï¼‰")
        self.sparse_retriever = SimpleBM25Retriever()
        self.document_store = None
        
        try:
            # ç¨ å¯†æ£€ç´¢æ¨¡å‹
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½ç¨ å¯†æ£€ç´¢æ¨¡å‹: {self.dense_model_name}")
            self.dense_tokenizer = AutoTokenizer.from_pretrained(
                self.dense_model_name,
                trust_remote_code=True,
                cache_dir="./model_cache"
            )
            self.dense_model = AutoModel.from_pretrained(
                self.dense_model_name,
                trust_remote_code=True,
                cache_dir="./model_cache"
            )
            print("âœ… ç¨ å¯†æ£€ç´¢æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ ç¨ å¯†æ£€ç´¢æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹...")
            try:
                # å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹
                backup_model = "./paraphrase-MiniLM-L3-v2"
                print(f"ğŸ”„ å°è¯•åŠ è½½å¤‡ç”¨æ¨¡å‹: {backup_model}")
                self.dense_tokenizer = AutoTokenizer.from_pretrained(
                    backup_model,
                    trust_remote_code=True,
                    cache_dir="./model_cache"
                )
                self.dense_model = AutoModel.from_pretrained(
                    backup_model,
                    trust_remote_code=True,
                    cache_dir="./model_cache"
                )
                print("âœ… å¤‡ç”¨ç¨ å¯†æ£€ç´¢æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e2:
                print(f"âš ï¸ å¤‡ç”¨æ¨¡å‹ä¹Ÿå¤±è´¥: {e2}")
                self.dense_tokenizer = None
                self.dense_model = None
        
        try:
            # äº¤å‰ç¼–ç å™¨ï¼ˆé‡æ’æ¨¡å‹ï¼‰
            from sentence_transformers import CrossEncoder
            self.cross_encoder = CrossEncoder(self.cross_encoder_name)
            print("âœ… äº¤å‰ç¼–ç å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ äº¤å‰ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.cross_encoder = None

    def add_documents(self, documents: List[Dict[str, Any]]):
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        for doc in documents:
            self.documents.append(doc)
            self.doc_metadata.append({
                'doc_id': doc.get('id', len(self.documents)),
                'content': doc.get('content', ''),
                'title': doc.get('title', ''),
                'metadata': doc.get('metadata', {})
            })
        
        # Haystack ä¸å¯ç”¨æ—¶ï¼Œç»™ SimpleBM25 æ„å»ºç´¢å¼•ï¼ˆéœ€è¦æœ‰ id å­—æ®µï¼‰
        if not hasattr(self, 'document_store'):
            simple_docs = []
            for i, doc in enumerate(documents):
                simple_docs.append({
                    'id': doc.get('id', i),
                    'content': doc.get('content', '')
                })
            self.sparse_retriever.add_documents(simple_docs)
        
        # è®¡ç®—ç¨ å¯†åµŒå…¥
        if self.dense_model is not None:
            self._compute_dense_embeddings()

    def _compute_dense_embeddings(self):
        """è®¡ç®—æ–‡æ¡£çš„ç¨ å¯†åµŒå…¥"""
        if not self.documents:
            return
        
        embeddings = []
        for doc in self.documents:
            content = doc.get('content', '')
            if content:
                embedding = self._encode_text(content)
                embeddings.append(embedding)
            else:
                embeddings.append(np.zeros(384))  # é»˜è®¤ç»´åº¦
        
        self.doc_embeddings = np.array(embeddings)

    def _encode_text(self, text: str) -> np.ndarray:
        """ç¼–ç æ–‡æœ¬"""
        if self.dense_tokenizer is None or self.dense_model is None:
            return np.random.randn(384)
        
        try:
            inputs = self.dense_tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            
            with torch.no_grad():
                outputs = self.dense_model(**inputs)
                # ä½¿ç”¨[CLS] tokençš„è¡¨ç¤º
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                return embedding.flatten()
                
        except Exception as e:
            print(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            return np.random.randn(384)

    def retrieve(self, query: str):
        """æ··åˆæ£€ç´¢"""
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥
        from models.evidence_cards import EvidenceCard, EvidenceCardCollection
        
        evidence_cards = EvidenceCardCollection()
        
        # 1. ç¨€ç–æ£€ç´¢
        sparse_results = self._sparse_retrieval(query)
        
        # 2. ç¨ å¯†æ£€ç´¢
        dense_results = self._dense_retrieval(query)
        
        # 3. ç»“æœèåˆ
        combined_results = self._combine_results(sparse_results, dense_results)
        
        # 4. äº¤å‰ç¼–ç å™¨é‡æ’
        if self.cross_encoder is not None and combined_results:
            try:
                reranked_results = self._cross_encoder_rerank(query, combined_results)
            except Exception as e:
                print(f"âš ï¸ äº¤å‰ç¼–ç å™¨é‡æ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ: {e}")
                reranked_results = combined_results
        else:
            reranked_results = combined_results
        
        # 5. è½¬æ¢ä¸ºè¯æ®å¡
        for i, result in enumerate(reranked_results[:self.final_top_k]):
            evidence_card = EvidenceCard(
                ocr_text=result['content'],
                bbox=result.get('bbox', [0, 0, 0, 0]),
                page_id=result.get('doc_id', f'doc_{i}'),
                confidence=result.get('score', 0.5),
                source_type=result.get('type', 'text')
            )
            
            evidence_card.set_importance_score(result.get('score', 0.5))
            evidence_card.set_relevance_score(result.get('relevance', 0.5))
            
            evidence_card.add_metadata('rank', i + 1)
            evidence_card.add_metadata('retrieval_method', result.get('method', 'hybrid'))
            
            evidence_cards.add_card(evidence_card)
        
        return evidence_cards

    def _sparse_retrieval(self, query: str) -> List[Dict[str, Any]]:
        """ç¨€ç–æ£€ç´¢ï¼ˆBM25ï¼‰"""
        try:
            if hasattr(self, 'document_store') and self.document_store is not None:
                # ä½¿ç”¨Haystackçš„BM25
                try:
                    results = self.sparse_retriever.retrieve(
                        query=query,
                        top_k=self.sparse_top_k
                    )
                    
                    sparse_results = []
                    for result in results:
                        sparse_results.append({
                            'content': result.content,
                            'score': result.score,
                            'doc_id': result.meta.get('doc_id', 'unknown'),
                            'method': 'sparse'
                        })
                    
                    return sparse_results
                except Exception as e:
                    print(f"âš ï¸ Haystack BM25æ£€ç´¢å¤±è´¥: {e}")
                    # å›é€€åˆ°ç®€åŒ–ç‰ˆBM25
                    results = self.sparse_retriever.retrieve(query, self.sparse_top_k)
                    # SimpleBM25 è¿”å› {'id','content','score','method'}ï¼Œä¸‹é¢ç»Ÿä¸€æˆ 'doc_id'
                    for r in results:
                        r['doc_id'] = r.get('doc_id', r.get('id', 'unknown'))
                    return results
            else:
                # ä½¿ç”¨ç®€åŒ–ç‰ˆBM25
                results = self.sparse_retriever.retrieve(query, self.sparse_top_k)
                # SimpleBM25 è¿”å› {'id','content','score','method'}ï¼Œä¸‹é¢ç»Ÿä¸€æˆ 'doc_id'
                for r in results:
                    r['doc_id'] = r.get('doc_id', r.get('id', 'unknown'))
                return results
                
        except Exception as e:
            print(f"ç¨€ç–æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _dense_retrieval(self, query: str) -> List[Dict[str, Any]]:
        """ç¨ å¯†æ£€ç´¢"""
        if self.dense_model is None or not self.doc_embeddings:
            return []
        
        try:
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self._encode_text(query)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for doc_embedding in self.doc_embeddings:
                similarity = self._cosine_similarity(query_embedding, doc_embedding)
                similarities.append(similarity)
            
            # æ’åº
            sorted_indices = np.argsort(similarities)[::-1]
            
            dense_results = []
            for i in sorted_indices[:self.dense_top_k]:
                if i < len(self.doc_metadata):
                    dense_results.append({
                        'content': self.doc_metadata[i]['content'],
                        'score': similarities[i],
                        'doc_id': self.doc_metadata[i]['doc_id'],
                        'method': 'dense'
                    })
            
            return dense_results
            
        except Exception as e:
            print(f"ç¨ å¯†æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _combine_results(self, sparse_results: List[Dict], 
                        dense_results: List[Dict]) -> List[Dict]:
        """èåˆç¨€ç–å’Œç¨ å¯†æ£€ç´¢ç»“æœ"""
        # åˆ›å»ºæ–‡æ¡£IDåˆ°ç»“æœçš„æ˜ å°„
        doc_scores = {}
        
        # å¤„ç†ç¨€ç–æ£€ç´¢ç»“æœ
        for result in sparse_results:
            doc_id = result['doc_id']
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {
                    'content': result['content'],
                    'doc_id': doc_id,
                    'sparse_score': result['score'],
                    'dense_score': 0.0,
                    'combined_score': 0.0
                }
            else:
                doc_scores[doc_id]['sparse_score'] = result['score']
        
        # å¤„ç†ç¨ å¯†æ£€ç´¢ç»“æœ
        for result in dense_results:
            doc_id = result['doc_id']
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {
                    'content': result['content'],
                    'doc_id': doc_id,
                    'sparse_score': 0.0,
                    'dense_score': result['score'],
                    'combined_score': 0.0
                }
            else:
                doc_scores[doc_id]['dense_score'] = result['score']
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        for doc_id, scores in doc_scores.items():
            # åŠ æƒèåˆ
            sparse_weight = 0.3
            dense_weight = 0.7
            scores['combined_score'] = (
                sparse_weight * scores['sparse_score'] +
                dense_weight * scores['dense_score']
            )
        
        # æ’åº
        combined_results = list(doc_scores.values())
        combined_results.sort(key=lambda x: x['combined_score'], reverse=True)
        
        return combined_results

    def _cross_encoder_rerank(self, query: str, 
                             candidates: List[Dict]) -> List[Dict]:
        """äº¤å‰ç¼–ç å™¨é‡æ’"""
        if self.cross_encoder is None or not candidates:
            print(f"âš ï¸ äº¤å‰ç¼–ç å™¨ä¸ºç©ºæˆ–å€™é€‰åˆ—è¡¨ä¸ºç©º: cross_encoder={self.cross_encoder is not None}, candidates={len(candidates) if candidates else 0}")
            return candidates
        
        print(f"ğŸ”„ å¼€å§‹äº¤å‰ç¼–ç å™¨é‡æ’: æŸ¥è¯¢='{query[:50]}...', å€™é€‰æ•°é‡={len(candidates)}")
        
        try:
            # å‡†å¤‡è¾“å…¥
            pairs = []
            valid_indices = []  # è®°å½•æœ‰æ•ˆçš„å€™é€‰ç´¢å¼•
            
            for i, candidate in enumerate(candidates):
                content = candidate.get('content', '')
                if content and len(content.strip()) > 0:  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©ºä¸”éç©ºå­—ç¬¦ä¸²
                    pairs.append([query, content])
                    valid_indices.append(i)
                else:
                    print(f"âš ï¸ å€™é€‰ {i} å†…å®¹ä¸ºç©ºæˆ–æ— æ•ˆ: {repr(content[:50]) if content else 'None'}")
            
            print(f"ğŸ“ å‡†å¤‡äº† {len(pairs)} ä¸ªæœ‰æ•ˆè¾“å…¥å¯¹, æœ‰æ•ˆç´¢å¼•: {valid_indices}")
            
            if not pairs:  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¾“å…¥å¯¹
                print(f"âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å€™é€‰å†…å®¹è¿›è¡Œé‡æ’ï¼Œè¿”å›åŸå§‹å€™é€‰")
                return candidates
            
            # è®¡ç®—é‡æ’åˆ†æ•°
            try:
                rerank_scores = self.cross_encoder.predict(pairs)
                if not isinstance(rerank_scores, (list, np.ndarray)):
                    print(f"âš ï¸ äº¤å‰ç¼–ç å™¨è¿”å›çš„åˆ†æ•°æ ¼å¼å¼‚å¸¸: {type(rerank_scores)}")
                    return candidates
            except Exception as e:
                print(f"âš ï¸ äº¤å‰ç¼–ç å™¨é¢„æµ‹å¤±è´¥: {e}")
                return candidates
            
            # ç¡®ä¿rerank_scoresæ˜¯åˆ—è¡¨
            if isinstance(rerank_scores, np.ndarray):
                rerank_scores = rerank_scores.tolist()
            
            # éªŒè¯åˆ†æ•°æ•°é‡
            if len(rerank_scores) != len(pairs):
                print(f"âš ï¸ åˆ†æ•°æ•°é‡({len(rerank_scores)})ä¸è¾“å…¥å¯¹æ•°é‡({len(pairs)})ä¸åŒ¹é…")
                return candidates
            
            # æ›´æ–°åˆ†æ•°
            for i, candidate in enumerate(candidates):
                if i in valid_indices:
                    # æ‰¾åˆ°å¯¹åº”çš„åˆ†æ•°ç´¢å¼•
                    score_idx = valid_indices.index(i)
                    if score_idx < len(rerank_scores):
                        candidate['rerank_score'] = rerank_scores[score_idx]
                        combined_score = candidate.get('combined_score', 0.0)
                        candidate['final_score'] = (
                            0.3 * combined_score +
                            0.7 * rerank_scores[score_idx]
                        )
                    else:
                        # å¦‚æœç´¢å¼•è¶Šç•Œï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°
                        candidate['rerank_score'] = 0.0
                        candidate['final_score'] = candidate.get('combined_score', 0.0)
                else:
                    # å¯¹äºæ— æ•ˆçš„å€™é€‰ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°
                    candidate['rerank_score'] = 0.0
                    candidate['final_score'] = candidate.get('combined_score', 0.0)
            
            # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
            candidates.sort(key=lambda x: x.get('final_score', 0.0), reverse=True)
            
            return candidates
            
        except Exception as e:
            print(f"äº¤å‰ç¼–ç å™¨é‡æ’å¤±è´¥: {e}")
            return candidates

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return dot_product / (norm1 * norm2)
        except Exception as e:
            print(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    def save_index(self, path: str):
        """ä¿å­˜ç´¢å¼•"""
        try:
            index_data = {
                'documents': self.documents,
                'metadata': self.doc_metadata,
                'embeddings': self.doc_embeddings.tolist() if hasattr(self.doc_embeddings, 'tolist') else []
            }
            
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ° {path}")
        except Exception as e:
            print(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")

    def load_index(self, path: str):
        """åŠ è½½ç´¢å¼•"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            self.documents = index_data.get('documents', [])
            self.doc_metadata = index_data.get('metadata', [])
            self.doc_embeddings = np.array(index_data.get('embeddings', []))
            
            print(f"âœ… ç´¢å¼•å·²ä» {path} åŠ è½½")
        except Exception as e:
            print(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_documents': len(self.documents),
            'total_embeddings': len(self.doc_embeddings) if hasattr(self.doc_embeddings, '__len__') else 0,
            'sparse_model': self.sparse_model_name,
            'dense_model': self.dense_model_name,
            'cross_encoder': self.cross_encoder_name
        }


class SimpleBM25Retriever:
    """
    ç®€åŒ–ç‰ˆBM25æ£€ç´¢å™¨ï¼ˆå½“Haystackä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
    """
    def __init__(self):
        self.documents = []
        self.term_freq = {}
        self.doc_freq = {}
        self.avg_doc_length = 0.0
        self.total_docs = 0
        
        # BM25å‚æ•°
        self.k1 = 1.2
        self.b = 0.75

    def add_documents(self, documents: List[Dict[str, Any]]):
        """æ·»åŠ æ–‡æ¡£"""
        self.documents = documents
        self._build_index()

    def _build_index(self):
        """æ„å»ºç´¢å¼•"""
        self.total_docs = len(self.documents)
        
        # è®¡ç®—è¯é¢‘å’Œæ–‡æ¡£é¢‘ç‡
        for doc in self.documents:
            content = doc.get('content', '')
            terms = content.split()
            doc_length = len(terms)
            self.avg_doc_length += doc_length
            
            # è¯é¢‘ç»Ÿè®¡
            for term in terms:
                if term not in self.term_freq:
                    self.term_freq[term] = {}
                if doc.get('id') not in self.term_freq[term]:
                    self.term_freq[term][doc.get('id')] = 0
                self.term_freq[term][doc.get('id')] += 1
                
                # æ–‡æ¡£é¢‘ç‡
                if term not in self.doc_freq:
                    self.doc_freq[term] = set()
                self.doc_freq[term].add(doc.get('id'))
        
        if self.total_docs > 0:
            self.avg_doc_length /= self.total_docs

    def retrieve(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """æ£€ç´¢"""
        query_terms = query.split()
        scores = {}
        
        for doc in self.documents:
            doc_id = doc.get('id')
            content = doc.get('content', '')
            doc_terms = content.split()
            doc_length = len(doc_terms)
            
            score = 0.0
            for term in query_terms:
                if term in self.term_freq and doc_id in self.term_freq[term]:
                    tf = self.term_freq[term][doc_id]
                    df = len(self.doc_freq[term])
                    
                    # BM25å…¬å¼
                    idf = np.log((self.total_docs - df + 0.5) / (df + 0.5))
                    numerator = tf * (self.k1 + 1)
                    denominator = tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)
                    
                    score += idf * numerator / denominator
            
            scores[doc_id] = score
        
        # æ’åº
        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        results = []
        for doc_id, score in sorted_docs[:top_k]:
            doc = next((d for d in self.documents if d.get('id') == doc_id), None)
            if doc:
                results.append({
                    'content': doc.get('content', ''),
                    'score': score,
                    'doc_id': doc_id,
                    'method': 'sparse'
                })
        
        return results
